#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Claude Response Evaluation Script
‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡∏≠‡∏á Claude
"""

import json
import sys
from typing import Dict, List, Any
import re

def evaluate_response(instruction: str, expected: str, actual: str) -> Dict[str, float]:
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö"""
    scores = {}
    
    # 1. Completeness (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
    expected_words = len(expected.split())
    actual_words = len(actual.split())
    if expected_words > 0:
        completeness = min(actual_words / expected_words, 1.0)
    else:
        completeness = 1.0 if actual_words > 0 else 0.0
    scores['completeness'] = completeness
    
    # 2. Relevance (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô instruction ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô response
    instruction_keywords = set(re.findall(r'\b\w+\b', instruction.lower()))
    response_keywords = set(re.findall(r'\b\w+\b', actual.lower()))
    if instruction_keywords:
        relevance = len(instruction_keywords & response_keywords) / len(instruction_keywords)
    else:
        relevance = 1.0
    scores['relevance'] = relevance
    
    # 3. Coherence (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
    if actual_words >= 10 and '.' in actual:
        coherence = 1.0
    elif actual_words >= 5:
        coherence = 0.7
    else:
        coherence = 0.3
    scores['coherence'] = coherence
    
    # 4. Helpfulness (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    helpful_words = ['help', 'assist', 'guide', 'explain', 'show', 'provide', 'suggest']
    helpful_count = sum(1 for word in helpful_words if word in actual.lower())
    helpfulness = min(helpful_count / 2, 1.0)
    scores['helpfulness'] = helpfulness
    
    # 5. Overall score
    scores['overall'] = sum(scores.values()) / len(scores)
    
    return scores

def main():
    if len(sys.argv) != 3:
        print("Usage: python claude_eval.py <test_data.jsonl> <claude_responses.jsonl>")
        sys.exit(1)
    
    test_file = sys.argv[1]
    responses_file = sys.argv[2]
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            test_data.append(json.loads(line.strip()))
    
    responses = []
    with open(responses_file, 'r', encoding='utf-8') as f:
        for line in f:
            responses.append(json.loads(line.strip()))
    
    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
    total_scores = {'completeness': 0, 'relevance': 0, 'coherence': 0, 'helpfulness': 0, 'overall': 0}
    
    for i, (test_item, response_item) in enumerate(zip(test_data, responses)):
        instruction = test_item.get('input', '')
        expected = test_item.get('output', '')
        actual = response_item.get('response', '')
        
        scores = evaluate_response(instruction, expected, actual)
        
        for metric, score in scores.items():
            total_scores[metric] += score
        
        if i < 5:  # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
            print(f"Sample {i+1}: {scores['overall']:.3f}")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    n = len(test_data)
    avg_scores = {metric: score/n for metric, score in total_scores.items()}
    
    print("\nüìä EVALUATION RESULTS:")
    print("=" * 40)
    for metric, score in avg_scores.items():
        print(f"{metric:12}: {score:.3f}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    results = {
        'total_samples': n,
        'average_scores': avg_scores,
        'evaluation_timestamp': str(datetime.now())
    }
    
    with open('evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Results saved to evaluation_results.json")

if __name__ == "__main__":
    from datetime import datetime
    main()
